{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tf_agents.environments import py_environment, tf_py_environment, suite_gym, suite_atari\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.trajectories import trajectory\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\q2the\\anaconda3\\envs\\mlenv\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_model = tf.keras.models.load_model(\"neural_network_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RPSLSEnvironment:\n",
    "    def __init__(self, model, max_moves=100):\n",
    "        self.model = model\n",
    "        self.action_space = 5  # Assuming 5 possible actions (rock, paper, scissors, lizard, spock)\n",
    "        self.max_moves = max_moves\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment\n",
    "        self.current_state = []  # Initialize the sequence of previous moves\n",
    "        self.num_moves = 0  # Initialize the number of moves\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Take a step in the environment\n",
    "        self.current_state.append(action)  # Update the sequence of previous moves\n",
    "        self.num_moves += 1  # Increment the number of moves\n",
    "        reward = self.calculate_reward(action)  # Calculate the reward based on the action\n",
    "        done = self.is_done()  # Check if the game is done\n",
    "        return self.current_state, reward, done, {}\n",
    "\n",
    "    def calculate_reward(self, action):\n",
    "        # Define your reward function based on the current state and the action taken\n",
    "        if len(self.current_state) < 2:\n",
    "            return -0.1  # Small negative reward for each move\n",
    "        else:\n",
    "            player_move = action\n",
    "            computer_move = self.current_state[-2]\n",
    "            # Define the game rules\n",
    "            if player_move == computer_move:\n",
    "                return 0  # Tie\n",
    "            elif (player_move - computer_move) % 5 in [1, 3]:\n",
    "                return 1  # Player wins\n",
    "            else:\n",
    "                return -1  # Player loses\n",
    "\n",
    "    def is_done(self):\n",
    "        # Define your termination condition for the game\n",
    "        return self.num_moves >= self.max_moves  # Terminate after a certain number of moves\n",
    "\n",
    "    def get_action(self, epsilon=0.1):\n",
    "        # Use the model to get the next action\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.choice(self.action_space)  # Choose a random action with probability epsilon\n",
    "        else:\n",
    "            return get_next_move(self.model, self.current_state)  # Choose the action with the highest predicted Q-value with probability 1-epsilon\n",
    "        \n",
    "# Initialize the environment with your loaded model\n",
    "env = RPSLSEnvironment(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of episodes to train for\n",
    "num_episodes = 1000\n",
    "\n",
    "# Define the discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "# Define the initial epsilon\n",
    "epsilon = 1.0\n",
    "\n",
    "# Define the minimum epsilon\n",
    "min_epsilon = 0.01\n",
    "\n",
    "# Define the epsilon decay rate\n",
    "epsilon_decay = 0.995\n",
    "\n",
    "# Initialize the list to store rewards\n",
    "rewards = []\n",
    "\n",
    "# Loop over episodes\n",
    "for i_episode in range(num_episodes):\n",
    "    # Reset the state\n",
    "    state = env.reset()\n",
    "\n",
    "    # Initialize the episode reward\n",
    "    episode_reward = 0\n",
    "\n",
    "    # Loop over steps in the episode\n",
    "    for t in range(env.max_moves):\n",
    "        # Choose an action\n",
    "        action = env.get_action(epsilon)\n",
    "\n",
    "        # Take a step\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Update the Q-values\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + gamma * np.max(loaded_model.predict(np.array(next_state).reshape(1, len(next_state), 1)))\n",
    "\n",
    "        target_f = loaded_model.predict(np.array(state).reshape(1, len(state), 1))\n",
    "        target_f[0][action] = target\n",
    "\n",
    "        # Train the model\n",
    "        loaded_model.fit(np.array(state).reshape(1, len(state), 1), target_f, epochs=1, verbose=0)\n",
    "\n",
    "        # Update the state and episode reward\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        # If the episode is done, break\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(min_epsilon, epsilon_decay * epsilon)\n",
    "\n",
    "    # Store the episode reward\n",
    "    rewards.append(episode_reward)\n",
    "\n",
    "    # Print the episode reward every 100 episodes\n",
    "    if i_episode % 100 == 0:\n",
    "        print(f\"Episode {i_episode}: {np.mean(rewards[-100:])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
